\documentclass[12pt,a4paper]{article}
\linespread{1.3}
%\usepackage[utf8]{inputenc}
\usepackage{a4wide}
\usepackage[official]{eurosym}

\usepackage{a4wide}
\usepackage[official]{eurosym}
\usepackage{latexsym}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{natbib}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage[title,titletoc]{appendix}

\usepackage{lscape}
%%----------------------------------------------------------------%%
\usepackage{tikz}

\usepackage{latexsym}
\usepackage{bbm}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage[title,titletoc]{appendix}
\usepackage[all]{xy}
\usepackage{caption}
\usepackage[scriptsize]{subfigure}
\usepackage{tabularx}
\usepackage{comment}
\usepackage{color}
\usepackage{verbatim}
\usepackage{multirow} %package needed for multirow placement in tables
\usepackage{subfigure}%option [scriptsize], for multiple figures within a figure environment
\usepackage{booktabs, threeparttable} %package for table environment spacing. using commands \toprule \midrule \bottomrule \cmidrule{c}{1-2}
%\usepackage[multiple]{footmisc}%for multiple footnotes at one point
\usepackage{mathtools}
%\usepackage[input-decimal-markers=.]{siunitx}

\definecolor{justblue}{RGB}{00,73,127}
\definecolor{geblue}{RGB}{0,100,150}
\usepackage{hyperref} % changes all (tex)refs to hyperrefs!
\hypersetup{breaklinks=true,
	 colorlinks=true,  %set =true for colored text instead; false for borders
   %citebordercolor=green,
	 citecolor=magenta,
%	 linkcolor=justblue,
	urlcolor=magenta}
\urlstyle{rm} %so it doesn't use a typewriter font for url's.

\begin{document}


\section{Project 1 - Algorithmic Dynamic Pricing}
\label{sec:1}

\subsection{Overview}
	\emph{Focus:} Algorithmic pricing recommendations and human responses, strategic behaviour and corresponding biases. Relevant for sticky prices/menu costs/inflation.

use hotel data, test cheap talk model

\subsection{Details from Ã–AW application}

\emph{Aim of Project 2.} In this project, I investigate the behavioral interaction between a human decision maker and an algorithm in a strategic situation to quantify how possibly misaligned incentives can change both the algorithmic predictions and recommendations and the final decision by the human decision maker. I will thus contribute empirically to the literature on human-algorithm interaction as captured by signalling games in game theory \citep{backus2019empirical}. In the setting I consider, incentives between the algorithm (agent), which provides price recommendations and a human decision maker (principal), who holds the final decision right to set prices for the good, do not have to be fully aligned.

\emph{Available data.} I have access to a large dataset used in \citet{GTW2021demandest}, originating from a revenue management platform which offers algorithmic dynamic price recommendations to hotel managers on the hotel room level that the individual managers (the clients) need to accept, adapt, or ignore manually. The panel consists of over 5 million observations, 130K price changes by hotel managers and more than 800K price recommendation updates.

\emph{Work steps.}
\begin{enumerate}
	\item \emph{Exploration of Recommendation-Decision Translation.} I will start with a descriptive analysis of the universe of algorithmic recommendations and pricing decisions both pooled over hotels and separately for each hotel, that is, for each (independent) human decision maker to uncover potential heterogeneity in response behavior.
Most importantly, I focus in the analysis of how algorithmic recommendations (advice) map into real prices set by human decision makers. My analysis thus addresses the pass-through of recommendations to pricing decisions, i.e.~ how final prices change in response to a change in the recommendations, both in size, level, rates and over time.

\item \emph{Signalling Model.} Based on the analysis of the pattern of recommendations and how human decision makers respond to these recommendations in the previous step, I will assess whether these patterns are consistent with patterns generated by existing cheap-talk or signalling models \citep[see][]{crawford1982strategic, sobel2013givingadvice, backus2019empirical}. A key component in this endeavour is to identify from the pass-through (price-response) data whether the recommendation system has strategic incentives to change behavior of the human decision maker. Even if algorithm and human agree on the objective to maximize revenue for the hotel, there might exist strategic reasons for the algorithm recommendation to deviate from truthful price recommendations. Suppose the human systematically adjusted prices by less than what the algorithm recommends. The algorithm could response to this and `nudge' the decision maker by exaggerating the recommendation, such that the revenue maximation objective can be reached. This concept is what \citet{cowgill2020algorithmic} call `algorithmic social engineering'. Weather apps, for instance, are well known to do exactly that: often, precipitation probability is systematically exaggerated to compensate for the users' wrong judgement of these probabilities, a phenomenon referred to as `wet bias' \citep{wetbias1}, albeit this was, to my knowledge, never analyzed from a game theoretical perceptive.

\item \emph{Experimental Study.} Based on the insight from the empirical analysis of the strategic interaction, I will design a laboratory experiment to follow up important assumptions and key mechanism of how recommendations are exaggerated and mapped into decisions which cannot be addressed by analysis of observational data. This will provide full `control' over the informational and institutional details and hence is a perfect method to investigate some of the key assumptions behind the communication game played by the algorithms and the human decision maker \citep[for an overview of human vs computer players in controlled economic experiments, see][]{march2019behavioral}.
%
Specifically, I want to investigate whether human receivers of algorithmic recommendations understand that the algorithm has an incentive to change its recommendation in order to `nudge' the human decision maker to make pricing decisions that lead to better outcomes. Additionally, I will find out how the human response to the recommendation changes depending on whether the algorithm is non-strategic, i.e.~only gives truthful recommendations or strategic, i.e.~distorts its recommendations according to the past human responses.

\end{enumerate}


\section{Project 2 - Human Forecasting}
\label{sec:human_forecasting}

\subsection{Overview}

\emph{Focus:} Human forecasting in a real world application and human forecasts as inputs to algorithmic forecasts.     
      
\textbf{Steps:}    
\begin{enumerate}
	
	\item Empirical analysis of our real world data (see \ref{subsec:data}) in the light of the BIN (bias, information, noise) model as described in \cite{satopaa2021bias}. The BIN model is a Bayesian approach to disentangle the three main components bias, information and noise of human forecasts. The BIN model of \cite{satopaa2021bias} is based on a continuous random variable $Z*$ and a corresponding human forecast $Z_0$ as follows:
	
\begin{equation}
  \begin{pmatrix}
  	Z*\\Z_0
  \end{pmatrix}
  = \mathcal{N}
  \begin{pmatrix}
	\begin{pmatrix}  
	\mu*\\\mu* + \mu_0
  	\end{pmatrix},
  	\begin{pmatrix}
		1 & \gamma \\
		\gamma_0 & \gamma_0 + \delta_0
  	\end{pmatrix}
  \end{pmatrix}  
\end{equation}
	
where	
	
\begin{center}	
\begin{tabular}{r c l}
Outcome: && $ Y = 1(Z* > 0)$ \\
Bias: && $\mu_0 = E[Z_0] - E[Z*]$ \\
Information: && $\gamma_0 = Cov(Z_0, Z*)$ \\
Noise: && $\delta_0 = Var(Z_0) - Cov(Z_0, Z*)$ \\
\end{tabular}
\end{center}
	
Note: Outcome is binary in the original paper, but its distribution is modelled as a continuous variable, Z, so we simply wont use the transformation via $1(Z* > 0)$. The model parameters are then estimated using real world data, and validated by simulating data using the obtained parameters.
	
What patterns in terms of bias, information and noise do we find in \emph{our} data? Difference between forecasters? 

Find a structural model that explains the empirical findings and estimate parameters based on the empirical data. Candidates are: 
	
	\begin{itemize}
		\item \cite{scheele2018designing}: Could describe a systematic upwards bias due to incentive structure (extra payments for sales forecasters only depend on sales, so overprediction and -production is tolerated to guarantee availability)
		\item \cite{ockenfels2014impulse}: an experimental adaption of the Newsvendor problem showing a behavioural pull-to-center bias.
	\end{itemize}

	\item Run experiments (lab/field/hybrid with firm forecasters) to find out how forecaster performance can be improved. Ideas for Treatments:

	\begin{itemize}
		\item treatments from \cite{bloom2022rationalizing}:
			\begin{itemize}
				\item providing past data (dashboard? how to assure that control group does not look at data? maybe first elicit status quo of how past data is used in a survey)
				\item forecasting training (how would this be done in our concrete case? is this as meaningful with professionals as it is with small entrepreneurs in the bloom study?)
			\end{itemize}
		\item based on \cite{ibrahim2021eliciting}: asking for private information adjustment instead of direct forecast (when human forecast is used as input for a forecasting algorithm). This should be especially promising for noise reduction. See below subsection \ref{subsec:pia}.
	\end{itemize}
	
\end{enumerate}

\subsection{Data}
\label{subsec:data}

Our corporate data sponsor is based in Austria and operates in the manufacturing industry globally. In the company, order forecasts are until now done manually and are used for supply-chain and production decisions. In total, the panel format data set we use contains XX Mio observations of various types of monthly order forecasts, realized orders and sales from a three-year period.     

The data is available at the following granularity: number of distinct values in parentheses: product group (2), product (200), geography identifier 1 (7), geography identifier 2 (10), (large scale) buyer (200). In practise, we observe 2000 unique combinations of these identifiers.     

We have the monthly forecasts for four forecaster types in the firm, namely sales forecaster, local planners, global planners and released demand planners (see Section \ref{subsec:institutional_setting} below). Forecasts range from 2 to 12 months ahead and are updated every month.    

Depending on the forecaster type, forecasts are done manually on a certain level (e.g. only whole product groups, not separate products) and split up automatically. They can, however, be adjusted manually up to the lowest granularity level of our data. Forecasts of each type are also done by different forecasters/forecasting teams in different geography units.

\subsection{Institutional details}
\label{subsec:institutional_setting}


In the company, order forecasts are done in monthly intervals by a sequence of forecasters: 

\begin{itemize}
\item In the beginning of each month, sales units provide their forecasts, followed by local demand planners and global demand planners. In the third week of the month, released demand planners confirm/adjust/combine everything to the final forecast.      
\item All forecasters can observe other forecasts that have already be done and use them as inputs. 
\item Forecasts are done for two months to 12 months ahead, so each forecast is updated ten times before it is final.
\end{itemize}

Descriptive Statistics:    




\subsection{Details on PIA}
\label{subsec:pia}

Assume a data-generating process: \[ Y = v + \sum_{i \in P \cup I} w_i X_i + \epsilon_i \]

$v$ ... intercept (baseline surgery duration)

$X_i$ ... explanatory vars (some private, some public)

$w_i$ ... linear weights ($\hat{=}$ regression coefficients)

$\epsilon$ ... random error (not explicable by $X_i$)

$I$ ... index set of private variables

$P$ ...  index set of public variables (i.e. known also to algorithm)     


We want to model this data-generating process and forecast it, so we need to find a way to access the private information $X_{i \in I}$ (private) via \emph{human inputs}:
	
\[M = \beta_0 + \beta_1 X_1 + ... + \beta_n X_n + \beta_{n+1} humanforecast\]

This gets better the more useful information is carried by the human forecast, i.e. the more correlated human forecast is with residuals of first n+1 terms

Human Forecasts can be a direct forecast (DF) or a private information adjustment (PIA) with:

\[DF = v + \sum_{i \in P \cup I} w_i X_i\]
\[PIA = \sum_{i \in I} w_i X_i\]

DF is a competing generic human forecast using all variables. To get PIA, forecasters are asked: consider an algorithm that has information $X_1$ to $X_n$. By how much would you correct this algorithm's forecast based on your additional information?

Consequently, (algorithmic) model predictions using these as inputs:

\[M_{DF} = \alpha_0 + \sum_{i \in P} \alpha_i X_i + \beta_{DF} DF\]

\[M_{PIA} = \gamma_0 + \sum_{i \in P} \gamma_i X_i + \beta_{PIA} PIA\]

Assuming consistent human forecasters (i.e. they apply a perfect linear model in their head like we all do in our daily life and do not make any errors), these two predictions are equal. However, the noisier human predictions are, the greater is the advantage of $M_{PIA}$.    

\textbf{Hypotheses:} 

$H_1$: \emph{All else being equal, a prediction model that
is calibrated using DF yields less accurate predictions than a
prediction model that is calibrated using PIA.} (if humans make random errors)


\[ \frac{1}{n}\sum_{n} rmse(M_{PIA, n}) < \frac{1}{n}\sum_{n} rmse(M_{DF_n})\]


rmse ... root mean square error

Secondary Hypotheses: 

\begin{itemize}

	\item $H_{2a}$: \emph{Inducing greater random error incorporating public information increases the DF-PIA gap.}
	\item $H_{2b}$: \emph{Inducing greater random error incorporating private information decreases the DF-PIA gap.}
	\item $H_{2c}$: \emph{Random error incorporating public information increases the DF-PIA gap more than random error incorporating private information.}

\end{itemize}

There remain some questions regarding the adaptation of the experimental setting in \citep{ibrahim2021eliciting} to our real world setting. In the original experiment, participants had to provide their adjustment \emph{before} they knew the algorithmic forecast, and did not have the opportunity to readjust the final algorithmic prediction. I see some problems doing it like this in a field experiment our setting:

\begin{itemize}
	\item the PIA must be the very first input. If the human forecaster see the algorithmic forecast before they provide their PIA, 
	\item right now there is already a sequence of human forecasters as described in Section  \ref{subsec:data}. So most forecasters already base their forecast on previous forecasts. We actually said that the algorithmic support happens after each step, so all but the very first forecasters have an algorithmic forecast already
	\item ALL forecasters without exception could treat last month's algorithmic predictions as anchor points
	\item forecasters that see the algorithmic forecast could simply readjust it until it matches their own hypothetical Direct Forecast, because, naturally, they think they are smarter than the algorithm and want to override it
	\ hard to distinguish between overall PIA and punctual adjustments that they already made for specific products.
	\item released demand planners will always have the final decision, so whenever an algorithmic forecast deviates to much from their intuition, the might readjust it until it matches their own hypothetical Direct Forecast. I doubt they would trust the algorithm sufficiently to accept a forecast that is unintuitive for them. We could, however, still use the preliminary algorithmic forecast for validation. Another possibility would be that forecasters have a wide range of forecasts the seems acceptable to them and would trust the algorithm as long as the forecast lies within the range. The problem could also be overcome if a different person makes the PIA input and the final optional adjustment.
	\item besides the PIA treatment: whenever we have the back-and-forth interaction resulting from our plan to provide preliminary algorithmic forecasts after each step of the forecasting procedure, we actually have a strategic situation, not any more a one-dimensional, purely technical problem  of increasing forecasting accuracy with human inputs.
\end{itemize}


\pagebreak
\renewcommand\refname{Bibliography}
\bibliography{bibliography}
\bibliographystyle{aer}

\end{document}
